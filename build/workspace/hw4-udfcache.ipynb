{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description \n",
    "\n",
    "This PySpark application analyzes scheduled aircraft landings to estimate the operational complexity of airspace management in busy airport regions. The system focuses on the top 10 most busy airports (by arrival volume) and treats each as the center of a regional airspace zone with a 200-mile radius.\n",
    "\n",
    "The final goal is to count of complexity of landings by measureing all possible sequences of landings assuming that the airport regions have atmost 5 runways. Essentially, given an aircraft arriving in the same airspace region within an hour interval, how many distinct landing sequences are possible.\n",
    "\n",
    "\n",
    "**Do not change the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edit the following code cell only for Question 2 and 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from math import radians, cos, sin, asin, sqrt, comb\n",
    "\n",
    "def parse_csv(path):\n",
    "    return sc.textFile(path).mapPartitionsWithIndex(\n",
    "        lambda idx, it: iter(list(it)[1:]) if idx == 0 else it\n",
    "    ).map(lambda line: next(csv.reader([line])))\n",
    "\n",
    "def parse_time(ts):\n",
    "    return datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "# Haversine distance in miles\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3956  # radius of Earth in miles\n",
    "    dlat, dlon = radians(lat2 - lat1), radians(lon2 - lon1)\n",
    "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
    "    return 2 * R * asin(sqrt(a))\n",
    "\n",
    "# Total number of landing sequence (at most 4 planes can land simultaneously)\n",
    "def sequence(n):\n",
    "    if n == 0: return 1\n",
    "    return sum(comb(n - 1, g - 1) * sequence(n - g) for g in range(1, min(5, n) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do not change the following code cells.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load airports and flights\n",
    "airports_rdd = parse_csv(\"/data/airports_data.csv\").map(\n",
    "    lambda row: (row[0], (float(row[3]), float(row[4])))  # airport_code -> (lat, lon)\n",
    ").cache()\n",
    "\n",
    "flights_rdd = parse_csv(\"/data/flights.csv\").filter(lambda x : x[9] !='').map(\n",
    "    lambda row: (row[5], parse_time(row[9]))  # (arrival_airport, actual_arrival)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get top 10 airports by arrival volume\n",
    "top_airports = flights_rdd.map(lambda x: (x[0], 1)) \\\n",
    "                          .reduceByKey(lambda a, b: a + b) \\\n",
    "                          .takeOrdered(10, key=lambda x: -x[1])\n",
    "top_airport_codes = set([a[0] for a in top_airports])\n",
    "top_airport_codes_bc = sc.broadcast(top_airport_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create (top_airport, [nearby_airports_within_radius])\n",
    "X = 200  # miles\n",
    "top_airports_rdd = airports_rdd.filter(lambda x: x[0] in top_airport_codes_bc.value)\n",
    "\n",
    "# Join each top airport with every airport to compute distance\n",
    "region_airport_map = top_airports_rdd.cartesian(airports_rdd) \\\n",
    "    .filter(lambda pair: haversine(pair[0][1][0], pair[0][1][1], pair[1][1][0], pair[1][1][1]) <= X) \\\n",
    "    .map(lambda pair: (pair[0][0], pair[1][0]))  # (region_center_airport, nearby_airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Invert map to (airport, [region_centers])\n",
    "airport_to_regions = region_airport_map.map(lambda x: (x[1], x[0])) \\\n",
    "                                       .groupByKey().mapValues(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Join flight arrivals with region mapping\n",
    "\n",
    "flights_by_region = flights_rdd.join(airport_to_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create hourly keys (region_center, hour) â†’ 1 for each region the arrival belongs to\n",
    "def assign_to_regions(record):\n",
    "    airport, (arrival_time, regions) = record\n",
    "    date = arrival_time.strftime(\"%Y-%m-%d\")\n",
    "    hour = arrival_time.strftime(\"%H\")\n",
    "    return [((region, date, hour), 1) for region in regions]\n",
    "\n",
    "region_hour_counts = flights_by_region.flatMap(assign_to_regions) \\\n",
    "                                      .reduceByKey(lambda x, y: x + y)\n",
    "# Output: (region_center_airport, date, hour) count_of_landings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "num_partitions = 50\n",
    "def region_hour_partitioner(key):\n",
    "    region, data, hour = key\n",
    "    return portable_hash(region) % num_partitions\n",
    "\n",
    "# Enforces 50 partitions based on the airport code.\n",
    "region_hour_counts = region_hour_counts.partitionBy(num_partitions, region_hour_partitioner) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer the questions below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hour_counts.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. [20 pts]\n",
    "\n",
    "Run the following cell and identify why the following takes prohibitively long time, or on some machines, never terminates? Provide concrete measurements about tasks, job runtinme, and other metrics to justify your answer. You are also welcome to inspect the output data by writing additional opeartors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Compute Count of Landing Sequences\n",
    "region_hour_date_seq = region_hour_counts.map(lambda x: (x[0][0], x[0][1], x[1], sequence(x[1])))\n",
    "# Output: (region_center_airport, date, hour, count_of_landings, landing_sequences)\n",
    "region_hour_date_seq.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 [40 pts]\n",
    "Often a small number of tasks in a stage are left with resoruce intensive data proccesing. In such cases, if possible, repartition the data can help improve the performance. Currenttly, the RDD ``region_hour_counts`` is partitioned into 50 partitions based on the key `Airport Region`. \n",
    "\n",
    "```python\n",
    "num_partitions = 50\n",
    "def region_hour_partitioner(key):\n",
    "    region, date, hour = key\n",
    "    return portable_hash(region) % num_partitions\n",
    "```\n",
    "\n",
    "Change the following cell to implement a different partitioning strategy such that the semantics of the program remain unchanged, but the job's runtime is reduced. You are **not allowed to use more than 50 partitions**. **Justify your changes in detail** by explaining:\n",
    "- Your observations from the original version,\n",
    "- What you learned from them,\n",
    "- What led you to design the solution you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the following partition stretagy \n",
    "from pyspark.rdd import portable_hash\n",
    "\n",
    "num_partitions = 50 \n",
    "def region_hour_partitioner(key):\n",
    "# IMPLEMENT THE PARTITION FUNCTION HERE\n",
    "\n",
    "\n",
    "\n",
    "region_airport_map = top_airports_rdd.cartesian(airports_rdd) \\\n",
    "    .filter(lambda pair: haversine(pair[0][1][0], pair[0][1][1], pair[1][1][0], pair[1][1][1]) <= X) \\\n",
    "    .map(lambda pair: (pair[0][0], pair[1][0]))\n",
    "airport_to_regions = region_airport_map.map(lambda x: (x[1], x[0])) \\\n",
    "    .groupByKey().mapValues(set)\n",
    "\n",
    "flights_rdd.join(airport_to_regions) \\\n",
    "           .flatMap(assign_to_regions) \\\n",
    "           .reduceByKey(lambda x, y: x + y) \\\n",
    "           .partitionBy(num_partitions, region_hour_partitioner) \\\n",
    "           .map(lambda x: (x[0][0], x[0][1], x[1], sequence(x[1]))) \\\n",
    "           .collect()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. [30pts]\n",
    "\n",
    "If the input column(s) to a UDF contain a lot of duplicate values, it can be beneficial to improve performance by ensuring that the UDF is only called once per distinct input value, rather than once per row. For example in our previous question, ``haversine`` function might be called mutiple times on the identical set of (lat,long) pairs, causing it to reperform the same measurement again. Your task is to implement *in-memory UDF caching* what is sometimes called memoization. Here is one example\n",
    "\n",
    "```python \n",
    "cache = {}\n",
    "\n",
    "def square(x):\n",
    "    if x in cache:\n",
    "        return cache[x]\n",
    "    result = x * x\n",
    "    cache[x] = result\n",
    "    return result\n",
    "\n",
    "print(square(4))  # Computes 4*4 = 16 and stores it\n",
    "print(square(4))  # Returns 16 from cache, doesn't recompute\n",
    "print(square(5))  # Computes 5*5 = 25 and stores it\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a [15pts]\n",
    "Implement *in-memory UDF caching*  function ``haversine_cache`` located in the code cell below. Comment the existing implementation.\n",
    "\n",
    "Rerun the job and report the the performance improvements if any. **Justify your changes in detail** by explaining:\n",
    "- Your observations from the original version,\n",
    "- What you learned from them,\n",
    "- What led you to design the solution you chose.\n",
    "\n",
    "\n",
    "Hint: Identify the stage where this function executes and inspect the tasks related to that stage. Do not forget to rerun the cell where you have made edits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the haversine_cache function here\n",
    "def haversine_cache(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "\n",
    "region_airport_map = top_airports_rdd.cartesian(airports_rdd) \\\n",
    "    .filter(lambda pair: haversine_cache(pair[0][1][0], pair[0][1][1], pair[1][1][0], pair[1][1][1]) <= X) \\\n",
    "    .map(lambda pair: (pair[0][0], pair[1][0]))\n",
    "airport_to_regions = region_airport_map.map(lambda x: (x[1], x[0])) \\\n",
    "    .groupByKey().mapValues(set)\n",
    "\n",
    "flights_rdd.join(airport_to_regions) \\\n",
    "           .flatMap(assign_to_regions) \\\n",
    "           .filter(lambda x : x[0][1] >=12 and x[0][1] <20)\\\n",
    "           .reduceByKey(lambda x, y: x + y) \\\n",
    "           .partitionBy(num_partitions, region_hour_partitioner) \\\n",
    "           .map(lambda x: (x[0][0], x[0][1], x[1], sequence(x[1]))) \\\n",
    "           .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.b [15pts]\n",
    "Implement *in-memory UDF caching* for function ``sequence_cache`` located in the cell below. Rerun the job below and report the the performance improvements if any. **Justify your changes in detail** by explaining:\n",
    "- Your observations from the original version,\n",
    "- What you learned from them,\n",
    "- What led you to design the solution you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your sequence Cache function here\n",
    "def sequence_cache(x):\n",
    "    \n",
    "region_airport_map = top_airports_rdd.cartesian(airports_rdd) \\\n",
    "    .filter(lambda pair: haversine(pair[0][1][0], pair[0][1][1], pair[1][1][0], pair[1][1][1]) <= X) \\\n",
    "    .map(lambda pair: (pair[0][0], pair[1][0]))\n",
    "airport_to_regions = region_airport_map.map(lambda x: (x[1], x[0])) \\\n",
    "    .groupByKey().mapValues(set)\n",
    "\n",
    "flights_rdd.join(airport_to_regions) \\\n",
    "           .flatMap(assign_to_regions) \\\n",
    "           .reduceByKey(lambda x, y: x + y) \\\n",
    "           .partitionBy(num_partitions, region_hour_partitioner) \\\n",
    "           .map(lambda x: (x[0][0], x[0][1], x[1], sequence_cache(x[1]))) \\\n",
    "           .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
